{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e40e21e",
   "metadata": {},
   "source": [
    "<h2>Retrieve satellite data using Google Earth Engine (GEE) corresponding to field data compiled by the Cape Cod Commission (CCC)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2af7911",
   "metadata": {},
   "source": [
    "<h4>Code accompanying \"Satellite imagery as a management tool for monitoring water clarity across freshwater ponds in Cape Cod, Massachusetts\" (Coffer et al., 2024, <em>Journal of Environmental Management</em>). \n",
    "Python code written by co-author Nikolay Nezlin.</h4>\n",
    "\n",
    "* Read and plot the geodatabase with pond locations from the Cape Cod Commission (CCC)  \n",
    "* Read the provided CCC field data to a dataframe \n",
    "* Make a Google Earth Engine (GEE) feature collection from the field data dataframe with points and polygons\n",
    "* Get mean and standard deviation of top of atmosphere (TOA) reflectances from satellite imagery using GEE \n",
    "    * For circular regions of certain radius  \n",
    "    * Removing pixels within a certain distance from shore to reduce adjacency effects\n",
    "* Return the feature collection, convert to a dataframe, and save as a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49242efc",
   "metadata": {},
   "source": [
    "**Step 1: Load all required packages. If a package has not yet been installed, run \"conda install [package name]\" from Anaconda Prompt.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca79569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import pandas as pd\n",
    "import geemap\n",
    "import datetime\n",
    "import pytz\n",
    "import folium\n",
    "import ee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc7c26f",
   "metadata": {},
   "source": [
    "**Step 2: Initialize GEE and change the project directory to the folder where the \"CCC_Ponds_Export_2020.gdb\" geodatabase and the \"SDD_all_samples.csv\" field dataset are stored. To initialize GEE, change 'your-gee-project' to the name of your GEE project. Then, the following code will open an internet window through which you must provide all permissions and copy the code generated on the last screen. Copy that wherever your code editor requests it (for Visual Studio code, the box is at the top of the window); once this is done once, the project should be initialized for future code execution. <br /><br />This is the only section of the code that should need to be updated by the user.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05610dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the following paths  \n",
    "proj_dir = '...'\n",
    "ccc_ponds_fn = 'Input_Data/Pond_Geodatabase/CCC_Ponds_Export_2020.gdb'\n",
    "sdd_dir = 'Input_Data/Field_Data/SDD_all_samples.csv'\n",
    "out_dir = 'Input_Data/Field_Data/'\n",
    "\n",
    "# Import and initialize Earth Engine package\n",
    "try:\n",
    "  # Trigger the authentication flow.\n",
    "  ee.Authenticate()\n",
    "  ee.Initialize(project='capecod-sdd')\n",
    "  print('The Earth Engine package initialized successfully!')\n",
    "except ee.EEException as e:\n",
    "  print('The Earth Engine package failed to initialize!')\n",
    "except:\n",
    "    print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c70faa4",
   "metadata": {},
   "source": [
    "**Step 3: Run this cell to define all needed functions for the remainder of the script. Note, running this cell won't produce any output, but will set the script up to be able to run the remainder of the code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf5afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display GEE objects using folium.\n",
    "def Mapdisplay(center, dicc, Tiles=\"OpensTreetMap\",zoom_start=10):\n",
    "    '''\n",
    "    :param center: Center of the map (Latitude and Longitude).\n",
    "    :param dicc: Earth Engine Geometries or Tiles dictionary\n",
    "    :param Tiles: Mapbox Bright,Mapbox Control Room,Stamen Terrain,Stamen Toner,stamenwatercolor,cartodbpositron.\n",
    "    :zoom_start: Initial zoom level for the map.\n",
    "    :return: A folium.Map object.\n",
    "    '''\n",
    "    mapViz = folium.Map(location=center,tiles=Tiles, zoom_start=zoom_start)\n",
    "    for k,v in dicc.items():\n",
    "        if ee.image.Image in [type(x) for x in v.values()]:\n",
    "            folium.TileLayer(\n",
    "                tiles = v[\"tile_fetcher\"].url_format,\n",
    "                attr  = 'Google Earth Engine',\n",
    "                overlay =True,\n",
    "                name  = k\n",
    "            ).add_to(mapViz)\n",
    "        else:\n",
    "            folium.GeoJson(\n",
    "                data = v,\n",
    "                name = k\n",
    "            ).add_to(mapViz)\n",
    "    mapViz.add_child(folium.LayerControl())\n",
    "    return mapViz\n",
    "\n",
    "# Transform EE date to str ```YYYYMMdd'T'HHmmss```, which wiill be used in file name\n",
    "def date_ee2str(date_ee, date_format=\"YYYYMMdd'T'HHmmss\"):\n",
    "    date_str = ee.Date(date_ee).format(date_format).getInfo()\n",
    "    return date_str\n",
    "\n",
    "# Transform EE date to python ```datetime```\n",
    "def date_ee2datetime(date_ee):\n",
    "    date_time = datetime.datetime.fromtimestamp(date_ee/1000, tz=pytz.UTC)\n",
    "    return date_time\n",
    "\n",
    "# Define a method for displaying Earth Engine image tiles to folium map.\n",
    "def add_ee_layer(self, ee_image_object, vis_params, name):\n",
    "  map_id_dict = ee.Image(ee_image_object).getMapId(vis_params)\n",
    "  folium.raster_layers.TileLayer(\n",
    "    tiles = map_id_dict['tile_fetcher'].url_format,\n",
    "    attr = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
    "    name = name,\n",
    "    overlay = True,\n",
    "    control = True\n",
    "  ).add_to(self)\n",
    "# Add EE drawing method to folium.\n",
    "folium.Map.add_ee_layer = add_ee_layer\n",
    "\n",
    "# Retrieve cloud mask data for each cloud mask type  \n",
    "def img_mask_L5789(image):\n",
    "    QA_RADSAT = image.select('QA_RADSAT')\n",
    "    radsatMask = QA_RADSAT.bitwiseAnd(0b1111111111).eq(0)\n",
    "    QA_PIXEL = image.select('QA_PIXEL')\n",
    "    fillMask = QA_PIXEL.bitwiseAnd(1<<0).eq(0)  # Image data\n",
    "    dilatedCloudMask = QA_PIXEL.bitwiseAnd(1<<1).eq(0)  # Dilated Cloud\n",
    "    cirrusMask = QA_PIXEL.bitwiseAnd(1<<2).eq(0)  # Cirrus\n",
    "    cloudMask = QA_PIXEL.bitwiseAnd(1<<3).eq(0)  # Cloud\n",
    "    cloudShadowMask = QA_PIXEL.bitwiseAnd(1<<4).eq(0)  # Cloud shadow\n",
    "    maskTot = radsatMask.And(dilatedCloudMask).And(cirrusMask).And(cloudMask).And(cloudShadowMask).rename('mask')\n",
    "    img_masked = image.updateMask(maskTot)\n",
    "    #\n",
    "    return img_masked\n",
    "\n",
    "# Mask Sentinel-2 data using opaqueCloudsMask and cirrusCloudsMask\n",
    "def img_mask_S2(image):\n",
    "    if ('QA60' in qa_bands_list):\n",
    "        QA60 = image.select('QA60')\n",
    "        opaqueCloudsMask = QA60.bitwiseAnd(1<<10).eq(0)\n",
    "        cirrusCloudsMask = QA60.bitwiseAnd(1<<11).eq(0)\n",
    "    else:\n",
    "        opaqueCloudsMask = ee.Image(1)\n",
    "        cirrusCloudsMask = ee.Image(1)\n",
    "    #\n",
    "    maskTot = opaqueCloudsMask.And(cirrusCloudsMask).rename('mask')\n",
    "    img_masked = image.updateMask(maskTot)\n",
    "    #\n",
    "    return img_masked\n",
    "\n",
    "# Apply cloud mask filtering \n",
    "def make_imgDatesDf(eeImgColl, offset2disp, count2disp):\n",
    "    imgDates_df = pd.DataFrame()\n",
    "    ImgCount = eeImgColl.size().getInfo()\n",
    "    if (offset2disp >= ImgCount):\n",
    "        return imgDates_df\n",
    "    if (count2disp + offset2disp > ImgCount):\n",
    "        count2disp = ImgCount - offset2disp\n",
    "    if (count2disp <= 0):\n",
    "        return imgDates_df\n",
    "    eeImgColl10 = ee.ImageCollection(eeImgColl.toList(count2disp, offset2disp))\n",
    "    imgDates_df['date_ee'] = eeImgColl10.aggregate_array(\"system:time_start\").getInfo()\n",
    "    imgDates_df['date_time'] = [date_ee2datetime(date_ee) for date_ee in imgDates_df['date_ee']]\n",
    "    if (sat_code in ['L5','L7','L8','L9']):\n",
    "        imgDates_df['CLOUD_COVER'] = [eeImgColl10.filterDate(int(imgDates_df['date_ee'].iat[k_img])).first(). \\\n",
    "                               get('CLOUD_COVER').getInfo() for k_img in range(len(imgDates_df))]\n",
    "    elif (sat_code == 'S2'):\n",
    "        imgDates_df['CLOUD_COVER'] = [eeImgColl10.filterDate(int(imgDates_df['date_ee'].iat[k_img])).first(). \\\n",
    "                               get('CLOUDY_PIXEL_PERCENTAGE').getInfo() for k_img in range(len(imgDates_df))]\n",
    "    else:\n",
    "        imgDates_df['CLOUD_COVER'] = -99\n",
    "    return imgDates_df\n",
    "\n",
    "# Display images\n",
    "def disp_images(eeImgColl, imgDates_df, stnCentroid, bands4rgb_dict, img_scale, stnFc):\n",
    "    vis_params = {\n",
    "        'bands': list(bands4rgb_dict.values()),\n",
    "        'min': [0,0,0],\n",
    "        'max': [1/img_scale,1/img_scale,1/img_scale]}\n",
    "    # Create a folium map object.\n",
    "    my_map = folium.Map(location = (stnCentroid[1],stnCentroid[0]), \\\n",
    "                    zoom_start=8, width=500,height=400)\n",
    "    # Add the the images to the map object.\n",
    "    for k_image in range(len(imgDates_df)):\n",
    "        ee_im1 = eeImgColl.filterDate(int(imgDates_df['date_ee'].iat[k_image])).first()\n",
    "        date_str = date_ee2str(int(imgDates_df['date_ee'].iat[k_image]), date_format=\"YYYY-MM-dd\")\n",
    "        cloud_cover = imgDates_df[\"CLOUD_COVER\"].iat[k_image]\n",
    "        my_map.add_ee_layer(ee_im1, vis_params, '%s %.1f' % (date_str, cloud_cover))\n",
    "\n",
    "    folium.GeoJson(data = stnFc.getInfo(), name='stations').add_to(my_map)\n",
    "    # Add a lat lon popup.\n",
    "    folium.LatLngPopup().add_to(my_map)\n",
    "    # Add a layer control panel to the map.\n",
    "    my_map.add_child(folium.LayerControl())\n",
    "    # Display the map.\n",
    "    display(my_map)\n",
    "\n",
    "# Remove the images with duplicated dates\n",
    "# From: https://gis.stackexchange.com/questions/336257/filter-out-duplicate-sentinel-2-images-form-earth-engine-image-collection-by-dat\n",
    "def detectar_duplicador(image):\n",
    "    esduplicado = ee.String(\"\")\n",
    "    numero = eeImgCollList.indexOf(image)\n",
    "    image1 = ee.Image(eeImgCollList.get(numero.add(1)))\n",
    "    # Compare the image(0) in the ImageCollection with the image(1) in the List\n",
    "    fecha1 = image.date().format(\"Y-M-d\")\n",
    "    fecha2 = image1.date().format(\"Y-M-d\")\n",
    "    estado = ee.Algorithms.IsEqual(fecha1,fecha2)\n",
    "    esduplicado = ee.String(ee.Algorithms.If(estado, \"duplicate\", \"no duplicate\"))\n",
    "    return image.set({\"duplicate\": esduplicado})\n",
    "\n",
    "# Retrieve satellite data for each relevant sensor \n",
    "def set_satellite(sat_code):\n",
    "    if (sat_code == 'L5'):\n",
    "        # Landsat-5 L1 (TOA)\n",
    "        # 1984-03-16T16:18:01Z–2012-05-05T17:54:06\n",
    "        ee_product = \"LANDSAT/LT05/C02/T1_TOA\"\n",
    "        bands_list = ['B1', 'B2', 'B3', 'B4', 'B5','B7']  # Optical bands\n",
    "        qa_bands_list = ['QA_PIXEL','QA_RADSAT'] # Do not use 'SR_CLOUD_QA'! LEDAPS quality conditions are wrong! \n",
    "        img_scale = 1\n",
    "        img_offset = 0\n",
    "        scale_m = 30  # Resolution for B1-B7 from \"LANDSAT_LC05_C02_T1_TOA\"\n",
    "        bands4rgb_dict = {'Red':'B3', 'Green':'B2', 'Blue':'B1'}\n",
    "        save_dir = os.path.join(proj_dir, out_dir)\n",
    "    elif (sat_code == 'L7'):\n",
    "        # Landsat-7 L1 (TOA)\n",
    "        # 1999-05-28T01:02:17Z–present\n",
    "        ee_product = \"LANDSAT/LE07/C02/T1_TOA\"\n",
    "        bands_list = ['B1', 'B2', 'B3', 'B4', 'B5','B7']  # Optical bands\n",
    "        qa_bands_list = ['QA_PIXEL','QA_RADSAT']\n",
    "        img_scale = 1\n",
    "        img_offset = 0\n",
    "        scale_m = 30  # Resolution for B1-B5 from \"LANDSAT_LC07_C02_T1_TOA\"\n",
    "        bands4rgb_dict = {'Red':'B4', 'Green':'B3', 'Blue':'B2'}\n",
    "        save_dir = os.path.join(proj_dir, out_dir)\n",
    "    elif (sat_code == 'L8'):\n",
    "        # Landsat-8 L1 (TOA)\n",
    "        # 2013-03-18T15:58:14Z–present\n",
    "        ee_product = \"LANDSAT/LC08/C02/T1_TOA\"\n",
    "        bands_list = ['B1', 'B2', 'B3', 'B4', 'B5','B6','B7']  # Optical bands\n",
    "        qa_bands_list = ['QA_PIXEL','QA_RADSAT']\n",
    "        img_scale = 1\n",
    "        img_offset = 0\n",
    "        scale_m = 30  # Resolution for B1-B7 from \"LANDSAT_LC08_C02_T1_TOA\"\n",
    "        bands4rgb_dict = {'Red':'B4', 'Green':'B3', 'Blue':'B2'}\n",
    "        save_dir = os.path.join(proj_dir, out_dir)\n",
    "    elif (sat_code == 'L9'):\n",
    "        # Landsat-9 L1 (TOA)\n",
    "        # 2021-10-31T00:00:00Z–present\n",
    "        ee_product = \"LANDSAT/LC09/C02/T1_TOA\"\n",
    "        bands_list = ['B1', 'B2', 'B3', 'B4', 'B5','B6','B7']  # Optical bands\n",
    "        qa_bands_list = ['QA_PIXEL','QA_RADSAT']\n",
    "        img_scale = 1\n",
    "        img_offset = 0\n",
    "        scale_m = 30  # Resolution for B1-B7 from \"LANDSAT_LC09_C02_T1_TOA\"\n",
    "        bands4rgb_dict = {'Red':'B4', 'Green':'B3', 'Blue':'B2'}\n",
    "        save_dir = os.path.join(proj_dir, out_dir)\n",
    "    elif (sat_code == 'S2'):    \n",
    "        # Sentinel-2 L1 (TOA)\n",
    "        # 2017-03-28T00:00:00Z - present\n",
    "        ee_product = \"COPERNICUS/S2_HARMONIZED\"  # L1C-TOA\n",
    "        bands_list = ['B1', 'B2', 'B3', 'B4', 'B5','B6','B7','B8','B8A','B9','B10','B11','B12']  # Optical bands\n",
    "        qa_bands_list = ['QA60'] \n",
    "        img_scale = 0.0001\n",
    "        img_offset = 0\n",
    "        scale_m = 10  \n",
    "        bands4rgb_dict = {'Red':'B4', 'Green':'B3', 'Blue':'B2'}\n",
    "        save_dir = os.path.join(proj_dir, out_dir)\n",
    "    else:\n",
    "        print('Wrong satellite code %s' % sat_code)\n",
    "        ee_product = \"\"  \n",
    "        bands_list = []  # Optical bands\n",
    "        qa_bands_list = [] \n",
    "        img_scale = 1\n",
    "        img_offset = 0\n",
    "        scale_m = 30  \n",
    "        bands4rgb_dict = {}\n",
    "        save_dir = ''\n",
    "    return ee_product,bands_list,qa_bands_list,img_scale,img_offset,scale_m,bands4rgb_dict,save_dir\n",
    "\n",
    "# Compute average Rrs \n",
    "def calcRrsIn(feature):\n",
    "    feature = feature.set({'RrsIn': feature.propertyNames().contains(bands_list[0]+'_mean')})\n",
    "    return feature\n",
    "\n",
    "# Add product ID as a variable \n",
    "def add_product_id(feature):\n",
    "    date_range = ee.Date(feature.get('date')).advance(feature.get('lagDays_mean'),'day').getRange('day')\n",
    "    productIdStr = ee.Algorithms.If(condition=ee.String(sat_code).equals('S2'), \\\n",
    "                                    trueCase='PRODUCT_ID', falseCase='LANDSAT_PRODUCT_ID')\n",
    "    PRODUCT_ID = eeImgColl.filterDate(date_range.start(),date_range.end()).first().get(productIdStr)\n",
    "    return feature.set({'PRODUCT_ID': PRODUCT_ID})\n",
    "\n",
    "# Calculate mean and Std.Dev of Rrs of <bands_list> in the circular region around the station location\n",
    "def calc_RrsMeanStd(feature):\n",
    "    # Filter the images collection within the date range\n",
    "    dateRange4stn = ee.DateRange(ee.Date(feature.get('date')).advance(-delta_days,'day'), \\\n",
    "                    ee.Date(feature.get('date')).advance(delta_days,'day'))\n",
    "    # Add to each image a band 'days_lag' and sort the collection \n",
    "    def add_days_lag(image):\n",
    "        lagDays = ee.Number(ee.Date(image.get(\"system:time_start\")) \\\n",
    "                     .difference(start=ee.Date(feature.get('date')), unit='day'))\n",
    "        lagAbsDays = lagDays.abs()\n",
    "        image = image.set({'lagAbsDays':lagAbsDays}) \\\n",
    "            .addBands(ee.Image.constant(lagDays).toFloat().rename('lagDays'))\n",
    "        return image\n",
    "    #\n",
    "    imgColl4stn = imgCollMasked.filterDate(dateRange4stn.start(), dateRange4stn.end()) \\\n",
    "                .filterBounds(feature.geometry()) \\\n",
    "                .map(detectar_duplicador).filter(ee.Filter.eq(\"duplicate\",\"no duplicate\")) \\\n",
    "                .map(add_days_lag).sort('lagAbsDays',False) \n",
    "    #\n",
    "    reducerMeanStDev = ee.Reducer.mean().combine(ee.Reducer.stdDev(),'',True)\n",
    "    sampleGeom = ee.Geometry.Point([feature.getNumber('longitude'), feature.getNumber('latitude')]) \\\n",
    "                                    .buffer(bufferR, maxError=1) \\\n",
    "                    .intersection(feature.geometry().buffer(bufferS, maxError=1), maxError=1)\n",
    "    feature_means = imgColl4stn.select(bands_list + ['lagDays']) \\\n",
    "                .mosaic().reduceRegion(**{ \\\n",
    "                      'reducer': reducerMeanStDev, \\\n",
    "                      'geometry': sampleGeom, \\\n",
    "                      'scale': scale_m, \\\n",
    "                      'maxPixels': 1e19 \\\n",
    "                })\n",
    "    return feature.set(feature_means)\n",
    "\n",
    "# Calculate the number of images available for each sample\n",
    "def calcNimg(feature):\n",
    "    dateRange4stn = ee.DateRange(ee.Date(feature.get('date')).advance(-delta_days,'day'), \\\n",
    "                    ee.Date(feature.get('date')).advance(delta_days,'day'))\n",
    "    imgColl4stn = imgCollMasked.filterDate(dateRange4stn.start(), dateRange4stn.end()) \\\n",
    "                .filterBounds(feature.geometry()) \\\n",
    "                .map(detectar_duplicador).filter(ee.Filter.eq(\"duplicate\",\"no duplicate\"))\n",
    "    feature = feature.set({'nImg': imgColl4stn.size()})\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a63137b",
   "metadata": {},
   "source": [
    "**Step 4: Read in input data, including the CCC geodatabase and field data CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d556556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in geodatabase \n",
    "ccc_ponds_gdf = gpd.read_file(os.path.join(proj_dir,ccc_ponds_fn))\n",
    "# Drop the objects without CCC_GIS_ID and Shape_Area\n",
    "ccc_ponds_gdf.dropna(axis=0, how='any', subset=['CCC_GIS_ID','Shape_Area'], inplace=True)\n",
    "ccc_ponds_gdf['CCC_GIS_ID'] = [s.strip() for s in ccc_ponds_gdf['CCC_GIS_ID']]\n",
    "ccc_ponds_gdf = ccc_ponds_gdf.loc[ccc_ponds_gdf['CCC_GIS_ID']!='']\n",
    "# Sort the objects\n",
    "ccc_ponds_gdf.sort_values(by='Shape_Area', inplace=True, ascending=False, ignore_index=True)\n",
    "# Transform to EPSG:4326\n",
    "ccc_ponds_gdf = ccc_ponds_gdf.to_crs('EPSG:4326')\n",
    "# Transform multipolygons to polygons\n",
    "ccc_ponds_gdf=ccc_ponds_gdf.explode(index_parts=True)\n",
    "# Read in field shapefile \n",
    "stn_df = pd.read_csv(os.path.join(proj_dir, sdd_dir))\n",
    "stn_df['date'] = pd.to_datetime(stn_df['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3979eb4",
   "metadata": {},
   "source": [
    "**Step 5: For each satellite, extract spectral information corresponding to field data and export coincident data as a CSV file. First, set parameters used for each satellite sensor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a8e77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The buffer around each sampling point (m)\n",
    "bufferR = 10\n",
    "# The buffer offshore (m, set to less than 0)\n",
    "bufferS = -30\n",
    "# The number of stations to process\n",
    "nStn2process = 10 \n",
    "# The maximum time lag between sample and image\n",
    "delta_days = 30  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f23f58",
   "metadata": {},
   "source": [
    "**<em>Sentinel-2 (S2) which was launched 28 March 2017 and is still operational.</em>** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98621e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the satellite code and start and end dates\n",
    "sat_code = 'S2'; date_start = '2017-03-28'; date_end = '2024-01-01'\n",
    "stn_dfSat = stn_df.loc[(stn_df['date']>=pd.to_datetime(date_start))& \\\n",
    "                     (stn_df['date']<=pd.to_datetime(date_end))].reset_index(drop=True).copy()\n",
    "nStn = len(stn_dfSat)\n",
    "# Subdirectory and file names\n",
    "save_fn = 'S2_Rrs0010C.csv'\n",
    "# Copy geodatabase with columns of interest \n",
    "ccc_ponds_gdf1 = ccc_ponds_gdf[['CCC_GIS_ID','geometry']].copy()\n",
    "# Set satellite parameters\n",
    "ee_product,bands_list,qa_bands_list,img_scale,img_offset,scale_m,bands4rgb_dict,save_dir = set_satellite(sat_code)\n",
    "colNames = ['PRODUCT_ID','CCC_GIS_ID','day_str','SDD (m)','lagDays_mean','lagDays_stdDev','latitude','longitude'] \\\n",
    "    + list(np.array([[s+'_mean',s+'_stdDev'] for s in bands_list]).flatten())\n",
    "# Extract Rrs\n",
    "indx_start = 0; indx_end = indx_start+nStn2process\n",
    "rrs_df = pd.DataFrame()\n",
    "while (indx_start<nStn):\n",
    "    stn_df1 = stn_dfSat[indx_start:indx_end].copy()\n",
    "    stn_gdf = gpd.GeoDataFrame(stn_df1.merge(ccc_ponds_gdf1, how='left', left_on='CCC_GIS_ID', \\\n",
    "                                        right_on='CCC_GIS_ID').dropna())\n",
    "    print('Processing observations %d - %d of %d (%d stations)' % (indx_start, indx_end, nStn, len(stn_gdf)))\n",
    "    # Create feature collection\n",
    "    stnFCwPolygons = geemap.geopandas_to_ee(stn_gdf, date='day_str', date_format='YYYY-MM-dd')\n",
    "    eeImgColl = ee.ImageCollection(ee_product) \\\n",
    "      .filterDate(ee.Date(str(np.min(stn_gdf['date']).strftime('%Y-%m-%d'))).advance(-delta_days,'day'), \\\n",
    "              ee.Date(str(np.max(stn_gdf['date']).strftime('%Y-%m-%d'))).advance(delta_days,'day')) \\\n",
    "      .filterBounds(stnFCwPolygons) \\\n",
    "      .sort(\"system:time_start\") \\\n",
    "      .select(bands_list + qa_bands_list)\n",
    "    # Apply cloud mask \n",
    "    imgCollMasked = eeImgColl.map(lambda image: img_mask_S2(image)).select(bands_list) \\\n",
    "        .map(lambda image: image.multiply(img_scale).add(img_offset).copyProperties(image, ['system:time_start']))\n",
    "    # Generate a List to compare dates\n",
    "    eeImgCollList = eeImgColl.toList(eeImgColl.size())\n",
    "    image = ee.Image(eeImgCollList.get(0))\n",
    "    # Add in the end of the list a dummy image\n",
    "    eeImgCollList = eeImgCollList.add(image)\n",
    "    stnWithImgFc = stnFCwPolygons.map(calcNimg).filter(ee.Filter.gt('nImg',0)).map(calc_RrsMeanStd) \\\n",
    "        .map(calcRrsIn).filter(ee.Filter.eq('RrsIn',True)).map(add_product_id)\n",
    "    if (stnWithImgFc.size().getInfo() > 0):\n",
    "        stnWithImgDf = geemap.ee_to_pandas(stnWithImgFc, col_names=colNames)\n",
    "        stnWithImgDf = stnWithImgDf.dropna().reset_index(drop=True)\n",
    "        rrs_df = pd.concat([rrs_df,stnWithImgDf])\n",
    "        print('    Found: %d images; Rrs calculated for %d stations' % \\\n",
    "                      (eeImgColl.size().getInfo(), len(stnWithImgDf)))\n",
    "    else:\n",
    "        print('    Found: %d images; Rrs calculated for 0 stations' % eeImgColl.size().getInfo())\n",
    "    indx_start = indx_end; indx_end = indx_start + nStn2process\n",
    "print('Done!')\n",
    "# Export to CSV file \n",
    "if (not os.path.exists(save_dir)):\n",
    "    os.mkdir(save_dir)\n",
    "rrs_df.to_csv(os.path.join(save_dir,save_fn), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac81e59",
   "metadata": {},
   "source": [
    "**<em>Landsat 5 (L5) was operational from 16 March 1984 to 5 May 2012.</em>** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b0a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the satellite code and start and end dates\n",
    "sat_code = 'L5'; date_start = '1984-03-16'; date_end = '2012-05-06'\n",
    "stn_dfSat = stn_df.loc[(stn_df['date']>=pd.to_datetime(date_start))& \\\n",
    "                     (stn_df['date']<=pd.to_datetime(date_end))].reset_index(drop=True).copy()\n",
    "nStn = len(stn_dfSat)\n",
    "# Subdirectory and file names\n",
    "save_fn = 'L5_Rrs0010C.csv'\n",
    "# Copy geodatabase with columns of interest \n",
    "ccc_ponds_gdf1 = ccc_ponds_gdf[['CCC_GIS_ID','geometry']].copy()\n",
    "# Set satellite parameters\n",
    "ee_product,bands_list,qa_bands_list,img_scale,img_offset,scale_m,bands4rgb_dict,save_dir = set_satellite(sat_code)\n",
    "colNames = ['PRODUCT_ID','CCC_GIS_ID','day_str','SDD (m)','lagDays_mean','lagDays_stdDev','latitude','longitude'] \\\n",
    "    + list(np.array([[s+'_mean',s+'_stdDev'] for s in bands_list]).flatten())\n",
    "# Extract Rrs\n",
    "indx_start = 0; indx_end = indx_start+nStn2process\n",
    "rrs_df = pd.DataFrame()\n",
    "while (indx_start<nStn):\n",
    "    stn_df1 = stn_dfSat[indx_start:indx_end].copy()\n",
    "    stn_gdf = gpd.GeoDataFrame(stn_df1.merge(ccc_ponds_gdf1, how='left', left_on='CCC_GIS_ID', \\\n",
    "                                        right_on='CCC_GIS_ID').dropna())\n",
    "    print('Processing observations %d - %d of %d (%d stations)' % (indx_start, indx_end, nStn, len(stn_gdf)))\n",
    "    # Create feature collection\n",
    "    stnFCwPolygons = geemap.geopandas_to_ee(stn_gdf, date='day_str', date_format='YYYY-MM-dd')\n",
    "    eeImgColl = ee.ImageCollection(ee_product) \\\n",
    "      .filterDate(ee.Date(str(np.min(stn_gdf['date']).strftime('%Y-%m-%d'))).advance(-delta_days,'day'), \\\n",
    "              ee.Date(str(np.max(stn_gdf['date']).strftime('%Y-%m-%d'))).advance(delta_days,'day')) \\\n",
    "      .filterBounds(stnFCwPolygons) \\\n",
    "      .sort(\"system:time_start\") \\\n",
    "      .select(bands_list + qa_bands_list)\n",
    "    # Apply cloud mask \n",
    "    imgCollMasked = eeImgColl.map(lambda image: img_mask_L5789(image)).select(bands_list) \\\n",
    "        .map(lambda image: image.multiply(img_scale).add(img_offset).copyProperties(image, ['system:time_start']))\n",
    "    # Generate a List to compare dates\n",
    "    eeImgCollList = eeImgColl.toList(eeImgColl.size())\n",
    "    image = ee.Image(eeImgCollList.get(0))\n",
    "    # Add in the end of the list a dummy image\n",
    "    eeImgCollList = eeImgCollList.add(image)\n",
    "    stnWithImgFc = stnFCwPolygons.map(calcNimg).filter(ee.Filter.gt('nImg',0)).map(calc_RrsMeanStd) \\\n",
    "        .map(calcRrsIn).filter(ee.Filter.eq('RrsIn',True)).map(add_product_id)\n",
    "    if (stnWithImgFc.size().getInfo() > 0):\n",
    "        stnWithImgDf = geemap.ee_to_pandas(stnWithImgFc, col_names=colNames)\n",
    "        stnWithImgDf = stnWithImgDf.dropna().reset_index(drop=True)\n",
    "        rrs_df = pd.concat([rrs_df,stnWithImgDf])\n",
    "        print('    Found: %d images; Rrs calculated for %d stations' % \\\n",
    "                      (eeImgColl.size().getInfo(), len(stnWithImgDf)))\n",
    "    else:\n",
    "        print('    Found: %d images; Rrs calculated for 0 stations' % eeImgColl.size().getInfo())\n",
    "    indx_start = indx_end; indx_end = indx_start + nStn2process\n",
    "print('Done!')\n",
    "# Export to CSV file \n",
    "if (not os.path.exists(save_dir)):\n",
    "    os.mkdir(save_dir)\n",
    "rrs_df.to_csv(os.path.join(save_dir,save_fn), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc299548",
   "metadata": {},
   "source": [
    "**<em>Landsat 7 (L7) was operational from 28 May 1999 to 6 April 2022.</em>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b994f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the satellite code and start and end dates\n",
    "sat_code = 'L7'; date_start = '1999-05-28'; date_end = '2024-01-01'\n",
    "stn_dfSat = stn_df.loc[(stn_df['date']>=pd.to_datetime(date_start))& \\\n",
    "                     (stn_df['date']<=pd.to_datetime(date_end))].reset_index(drop=True).copy()\n",
    "nStn = len(stn_dfSat)\n",
    "# Subdirectory and file names\n",
    "save_fn = 'L7_Rrs0010C.csv'\n",
    "# Copy geodatabase with columns of interest \n",
    "ccc_ponds_gdf1 = ccc_ponds_gdf[['CCC_GIS_ID','geometry']].copy()\n",
    "# Set satellite parameters\n",
    "ee_product,bands_list,qa_bands_list,img_scale,img_offset,scale_m,bands4rgb_dict,save_dir = set_satellite(sat_code)\n",
    "colNames = ['PRODUCT_ID','CCC_GIS_ID','day_str','SDD (m)','lagDays_mean','lagDays_stdDev','latitude','longitude'] \\\n",
    "    + list(np.array([[s+'_mean',s+'_stdDev'] for s in bands_list]).flatten())\n",
    "# Extract Rrs\n",
    "indx_start = 0; indx_end = indx_start+nStn2process\n",
    "rrs_df = pd.DataFrame()\n",
    "while (indx_start<nStn):\n",
    "    stn_df1 = stn_dfSat[indx_start:indx_end].copy()\n",
    "    stn_gdf = gpd.GeoDataFrame(stn_df1.merge(ccc_ponds_gdf1, how='left', left_on='CCC_GIS_ID', \\\n",
    "                                        right_on='CCC_GIS_ID').dropna())\n",
    "    print('Processing observations %d - %d of %d (%d stations)' % (indx_start, indx_end, nStn, len(stn_gdf)))\n",
    "    # Create feature collection\n",
    "    stnFCwPolygons = geemap.geopandas_to_ee(stn_gdf, date='day_str', date_format='YYYY-MM-dd')\n",
    "    eeImgColl = ee.ImageCollection(ee_product) \\\n",
    "      .filterDate(ee.Date(str(np.min(stn_gdf['date']).strftime('%Y-%m-%d'))).advance(-delta_days,'day'), \\\n",
    "              ee.Date(str(np.max(stn_gdf['date']).strftime('%Y-%m-%d'))).advance(delta_days,'day')) \\\n",
    "      .filterBounds(stnFCwPolygons) \\\n",
    "      .sort(\"system:time_start\") \\\n",
    "      .select(bands_list + qa_bands_list)\n",
    "    # Apply cloud mask \n",
    "    imgCollMasked = eeImgColl.map(lambda image: img_mask_L5789(image)).select(bands_list) \\\n",
    "        .map(lambda image: image.multiply(img_scale).add(img_offset).copyProperties(image, ['system:time_start']))\n",
    "    # Generate a List to compare dates\n",
    "    eeImgCollList = eeImgColl.toList(eeImgColl.size())\n",
    "    image = ee.Image(eeImgCollList.get(0))\n",
    "    # Add in the end of the list a dummy image\n",
    "    eeImgCollList = eeImgCollList.add(image)\n",
    "    stnWithImgFc = stnFCwPolygons.map(calcNimg).filter(ee.Filter.gt('nImg',0)).map(calc_RrsMeanStd) \\\n",
    "        .map(calcRrsIn).filter(ee.Filter.eq('RrsIn',True)).map(add_product_id)\n",
    "    if (stnWithImgFc.size().getInfo() > 0):\n",
    "        stnWithImgDf = geemap.ee_to_pandas(stnWithImgFc, col_names=colNames)\n",
    "        stnWithImgDf = stnWithImgDf.dropna().reset_index(drop=True)\n",
    "        rrs_df = pd.concat([rrs_df,stnWithImgDf])\n",
    "        print('    Found: %d images; Rrs calculated for %d stations' % \\\n",
    "                      (eeImgColl.size().getInfo(), len(stnWithImgDf)))\n",
    "    else:\n",
    "        print('    Found: %d images; Rrs calculated for 0 stations' % eeImgColl.size().getInfo())\n",
    "    indx_start = indx_end; indx_end = indx_start + nStn2process\n",
    "print('Done!')\n",
    "# Export to CSV file \n",
    "if (not os.path.exists(save_dir)):\n",
    "    os.mkdir(save_dir)\n",
    "rrs_df.to_csv(os.path.join(save_dir,save_fn), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4479417e",
   "metadata": {},
   "source": [
    "**<em>Landsat 8 (L8) which was launched 18 March 2013 and is still operational.</em>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36ee792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the satellite code and start and end dates\n",
    "sat_code = 'L8'; date_start = '2013-03-18'; date_end = '2024-01-01'\n",
    "stn_dfSat = stn_df.loc[(stn_df['date']>=pd.to_datetime(date_start))& \\\n",
    "                     (stn_df['date']<=pd.to_datetime(date_end))].reset_index(drop=True).copy()\n",
    "nStn = len(stn_dfSat)\n",
    "# Subdirectory and file names\n",
    "save_fn = 'L8_Rrs0010C.csv'\n",
    "# Copy geodatabase with columns of interest \n",
    "ccc_ponds_gdf1 = ccc_ponds_gdf[['CCC_GIS_ID','geometry']].copy()\n",
    "# Set satellite parameters\n",
    "ee_product,bands_list,qa_bands_list,img_scale,img_offset,scale_m,bands4rgb_dict,save_dir = set_satellite(sat_code)\n",
    "colNames = ['PRODUCT_ID','CCC_GIS_ID','day_str','SDD (m)','lagDays_mean','lagDays_stdDev','latitude','longitude'] \\\n",
    "    + list(np.array([[s+'_mean',s+'_stdDev'] for s in bands_list]).flatten())\n",
    "# Extract Rrs\n",
    "indx_start = 0; indx_end = indx_start+nStn2process\n",
    "rrs_df = pd.DataFrame()\n",
    "while (indx_start<nStn):\n",
    "    stn_df1 = stn_dfSat[indx_start:indx_end].copy()\n",
    "    stn_gdf = gpd.GeoDataFrame(stn_df1.merge(ccc_ponds_gdf1, how='left', left_on='CCC_GIS_ID', \\\n",
    "                                        right_on='CCC_GIS_ID').dropna())\n",
    "    print('Processing observations %d - %d of %d (%d stations)' % (indx_start, indx_end, nStn, len(stn_gdf)))\n",
    "    # Create feature collection\n",
    "    stnFCwPolygons = geemap.geopandas_to_ee(stn_gdf, date='day_str', date_format='YYYY-MM-dd')\n",
    "    eeImgColl = ee.ImageCollection(ee_product) \\\n",
    "      .filterDate(ee.Date(str(np.min(stn_gdf['date']).strftime('%Y-%m-%d'))).advance(-delta_days,'day'), \\\n",
    "              ee.Date(str(np.max(stn_gdf['date']).strftime('%Y-%m-%d'))).advance(delta_days,'day')) \\\n",
    "      .filterBounds(stnFCwPolygons) \\\n",
    "      .sort(\"system:time_start\") \\\n",
    "      .select(bands_list + qa_bands_list)\n",
    "    # Apply cloud mask \n",
    "    imgCollMasked = eeImgColl.map(lambda image: img_mask_L5789(image)).select(bands_list) \\\n",
    "        .map(lambda image: image.multiply(img_scale).add(img_offset).copyProperties(image, ['system:time_start']))\n",
    "    # Generate a List to compare dates\n",
    "    eeImgCollList = eeImgColl.toList(eeImgColl.size())\n",
    "    image = ee.Image(eeImgCollList.get(0))\n",
    "    # Add in the end of the list a dummy image\n",
    "    eeImgCollList = eeImgCollList.add(image)\n",
    "    stnWithImgFc = stnFCwPolygons.map(calcNimg).filter(ee.Filter.gt('nImg',0)).map(calc_RrsMeanStd) \\\n",
    "        .map(calcRrsIn).filter(ee.Filter.eq('RrsIn',True)).map(add_product_id)\n",
    "    if (stnWithImgFc.size().getInfo() > 0):\n",
    "        stnWithImgDf = geemap.ee_to_pandas(stnWithImgFc, col_names=colNames)\n",
    "        stnWithImgDf = stnWithImgDf.dropna().reset_index(drop=True)\n",
    "        rrs_df = pd.concat([rrs_df,stnWithImgDf])\n",
    "        print('    Found: %d images; Rrs calculated for %d stations' % \\\n",
    "                      (eeImgColl.size().getInfo(), len(stnWithImgDf)))\n",
    "    else:\n",
    "        print('    Found: %d images; Rrs calculated for 0 stations' % eeImgColl.size().getInfo())\n",
    "    indx_start = indx_end; indx_end = indx_start + nStn2process\n",
    "print('Done!')\n",
    "# Export to CSV file \n",
    "if (not os.path.exists(save_dir)):\n",
    "    os.mkdir(save_dir)\n",
    "rrs_df.to_csv(os.path.join(save_dir,save_fn), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51433605",
   "metadata": {},
   "source": [
    "**<em>Landsat 9 (L9) which was launched 31 October 2021 and is still operational.</em>** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720593ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the satellite code and start and end dates\n",
    "sat_code = 'L9'; date_start = '2021-10-31'; date_end = '2024-01-01'\n",
    "stn_dfSat = stn_df.loc[(stn_df['date']>=pd.to_datetime(date_start))& \\\n",
    "                     (stn_df['date']<=pd.to_datetime(date_end))].reset_index(drop=True).copy()\n",
    "nStn = len(stn_dfSat)\n",
    "# Subdirectory and file names\n",
    "save_fn = 'L9_Rrs0010C.csv'\n",
    "# Copy geodatabase with columns of interest \n",
    "ccc_ponds_gdf1 = ccc_ponds_gdf[['CCC_GIS_ID','geometry']].copy()\n",
    "# Set satellite parameters\n",
    "ee_product,bands_list,qa_bands_list,img_scale,img_offset,scale_m,bands4rgb_dict,save_dir = set_satellite(sat_code)\n",
    "colNames = ['PRODUCT_ID','CCC_GIS_ID','day_str','SDD (m)','lagDays_mean','lagDays_stdDev','latitude','longitude'] \\\n",
    "    + list(np.array([[s+'_mean',s+'_stdDev'] for s in bands_list]).flatten())\n",
    "# Extract Rrs\n",
    "indx_start = 0; indx_end = indx_start+nStn2process\n",
    "rrs_df = pd.DataFrame()\n",
    "while (indx_start<nStn):\n",
    "    stn_df1 = stn_dfSat[indx_start:indx_end].copy()\n",
    "    stn_gdf = gpd.GeoDataFrame(stn_df1.merge(ccc_ponds_gdf1, how='left', left_on='CCC_GIS_ID', \\\n",
    "                                        right_on='CCC_GIS_ID').dropna())\n",
    "    print('Processing observations %d - %d of %d (%d stations)' % (indx_start, indx_end, nStn, len(stn_gdf)))\n",
    "    # Create feature collection\n",
    "    stnFCwPolygons = geemap.geopandas_to_ee(stn_gdf, date='day_str', date_format='YYYY-MM-dd')\n",
    "    eeImgColl = ee.ImageCollection(ee_product) \\\n",
    "      .filterDate(ee.Date(str(np.min(stn_gdf['date']).strftime('%Y-%m-%d'))).advance(-delta_days,'day'), \\\n",
    "              ee.Date(str(np.max(stn_gdf['date']).strftime('%Y-%m-%d'))).advance(delta_days,'day')) \\\n",
    "      .filterBounds(stnFCwPolygons) \\\n",
    "      .sort(\"system:time_start\") \\\n",
    "      .select(bands_list + qa_bands_list)\n",
    "    # Apply cloud mask \n",
    "    imgCollMasked = eeImgColl.map(lambda image: img_mask_L5789(image)).select(bands_list) \\\n",
    "        .map(lambda image: image.multiply(img_scale).add(img_offset).copyProperties(image, ['system:time_start']))\n",
    "    # Generate a List to compare dates\n",
    "    eeImgCollList = eeImgColl.toList(eeImgColl.size())\n",
    "    image = ee.Image(eeImgCollList.get(0))\n",
    "    # Add in the end of the list a dummy image\n",
    "    eeImgCollList = eeImgCollList.add(image)\n",
    "    stnWithImgFc = stnFCwPolygons.map(calcNimg).filter(ee.Filter.gt('nImg',0)).map(calc_RrsMeanStd) \\\n",
    "        .map(calcRrsIn).filter(ee.Filter.eq('RrsIn',True)).map(add_product_id)\n",
    "    if (stnWithImgFc.size().getInfo() > 0):\n",
    "        stnWithImgDf = geemap.ee_to_pandas(stnWithImgFc, col_names=colNames)\n",
    "        stnWithImgDf = stnWithImgDf.dropna().reset_index(drop=True)\n",
    "        rrs_df = pd.concat([rrs_df,stnWithImgDf])\n",
    "        print('    Found: %d images; Rrs calculated for %d stations' % \\\n",
    "                      (eeImgColl.size().getInfo(), len(stnWithImgDf)))\n",
    "    else:\n",
    "        print('    Found: %d images; Rrs calculated for 0 stations' % eeImgColl.size().getInfo())\n",
    "    indx_start = indx_end; indx_end = indx_start + nStn2process\n",
    "print('Done!')\n",
    "# Export to CSV file \n",
    "if (not os.path.exists(save_dir)):\n",
    "    os.mkdir(save_dir)\n",
    "rrs_df.to_csv(os.path.join(save_dir,save_fn), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
