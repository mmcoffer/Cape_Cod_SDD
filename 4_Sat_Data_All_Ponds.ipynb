{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11af7120",
   "metadata": {},
   "source": [
    "<h2>Retrieve satellite data using Google Earth Engine (GEE) corresponding to all resolvable ponds</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552390b0",
   "metadata": {},
   "source": [
    "<h4>Code accompanying \"Satellite imagery as a management tool for monitoring water clarity across freshwater ponds in Cape Cod, Massachusetts\" (Coffer et al., 2024, <em>Journal of Environmental Management</em>). \n",
    "Python code written by co-author Nikolay Nezlin.</h4>\n",
    "\n",
    "* Select several ponds\n",
    "* Select one satellite (GEE product)\n",
    "* For each pond:\n",
    "    * Calculate the pond center - the most offshore location  \n",
    "    * Make ```df``` with times of observations with cloud cover ```<threshold```  \n",
    "    * Extract time series of Rrs in the pond center  \n",
    "    * Export ```df``` with Rrs as CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ca1ede",
   "metadata": {},
   "source": [
    "**Step 1: Load all required packages. If a package has not yet been installed, run \"conda install [package name]\" from Anaconda Prompt.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca93df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ee\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import pandas as pd\n",
    "import geemap\n",
    "import datetime\n",
    "import pytz\n",
    "from shapely.geometry import Polygon, Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5da262",
   "metadata": {},
   "source": [
    "**Step 2: Initialize GEE and change the project directory to the folder where the \"CCC_Ponds_Export_2020.gdb\" geodatabase is stored. To initialize GEE, change 'your-gee-project' to the name of your GEE project. Then, the following code will open an internet window through which you must provide all permissions and copy the code generated on the last screen. Copy that wherever your code editor requests it (for Visual Studio code, the box is at the top of the window); once this is done once, the project should be initialized for future code execution.<br /><br />Here you can also change the date range of interest. Research efforts initially focused on the monitoring season, which spans 1 April through 31 October of each year. This code is capable of running all timespans for each sensor, but this will be quite computationally intensive and can take many days to run. Just running a single monitoring season such as for the example below will take about a full day to run. The following range of dates is available for each satellite sensor of interest:<br /><li>Landsat 5 (1 March 1984 - 5 June 2013)<br /><li>Landsat 7 (15 April 1999 - 6 April 2022)<br /><li>Landsat 8 (11 February 2013 - present)<br /><li>Landsat 9 (27 September 2021 - present)<br /><li>Sentinel-2 (23 June 2015 - present)<br /><br />This is the only section of the code that should need to be updated by the user.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d8a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the following paths  \n",
    "proj_dir = '...'\n",
    "ccc_ponds_fn = 'Input_Data/Pond_Geodatabase/CCC_Ponds_Export_2020.gdb'\n",
    "out_dir = 'Input_Data/Reflectance_Timeseries/'\n",
    "if (not os.path.exists(os.path.join(proj_dir,out_dir))):\n",
    "        os.mkdir(os.path.join(proj_dir,out_dir))\n",
    "\n",
    "# Set the range of dates to be quiered (YYYY-MM-DD)\n",
    "date_start = '2023-04-01'\n",
    "date_end = '2023-10-31'\n",
    "\n",
    "# Import and initialize Earth Engine package\n",
    "try:\n",
    "  # Trigger the authentication flow.\n",
    "  ee.Authenticate()\n",
    "  ee.Initialize(project='capecod-sdd')\n",
    "  print('The Earth Engine package initialized successfully!')\n",
    "except ee.EEException as e:\n",
    "  print('The Earth Engine package failed to initialize!')\n",
    "except:\n",
    "    print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1039a5d0",
   "metadata": {},
   "source": [
    "**Step 3: Run this cell to define all needed functions for the remainder of the script. Note, running this cell won't produce any output, but will set the script up to be able to run the remainder of the code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5237bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the point at the maximum distance from shore\n",
    "def findPmaxOffshore(pond_geom):\n",
    "    # Boundaries\n",
    "    pond_bounds = pond_geom.bounds\n",
    "    # XY grid\n",
    "    x1d = np.linspace(pond_bounds[0], pond_bounds[2])\n",
    "    y1d = np.linspace(pond_bounds[1], pond_bounds[3])\n",
    "    xx, yy = np.meshgrid(x1d, y1d)\n",
    "    # Create geodataframe\n",
    "    grid_gdf = gpd.GeoDataFrame(geometry=gpd.points_from_xy(x=xx.reshape(-1), y=yy.reshape(-1)), crs='EPSG:26919')\n",
    "    # Points in pond\n",
    "    grid_gdf = grid_gdf.iloc[[pond_geom.contains(g) for g in grid_gdf.geometry]]\n",
    "    # Create geometry of land around the pond\n",
    "    lat_point_list = np.asarray([pond_bounds[1], pond_bounds[1], pond_bounds[3], pond_bounds[3], pond_bounds[1]])\n",
    "    lon_point_list = np.asarray([pond_bounds[0], pond_bounds[2], pond_bounds[2], pond_bounds[0], pond_bounds[0]])\n",
    "    land_geom = Polygon(zip(lon_point_list, lat_point_list)).difference(pond_geom)\n",
    "    # Distance from land\n",
    "    grid_gdf['dist'] = [land_geom.distance(gg) for gg in grid_gdf.geometry]\n",
    "    # Find maximum\n",
    "    max_offshore_point = grid_gdf.loc[grid_gdf['dist']==grid_gdf['dist'].max()]['geometry']\n",
    "    return max_offshore_point\n",
    "\n",
    "# Set parameters for the given satellite \n",
    "def set_satellite(sat_code):\n",
    "    if (sat_code == 'L5'):\n",
    "        # Landsat-5 L1 (TOA)\n",
    "        # 1984-03-16T16:18:01Z–2012-05-05T17:54:06\n",
    "        ee_product = \"LANDSAT/LT05/C02/T1_TOA\"\n",
    "        bands_list = ['B1', 'B2', 'B3', 'B4', 'B5','B7']  # Optical bands\n",
    "        qa_bands_list = ['QA_PIXEL','QA_RADSAT'] # Do not use 'SR_CLOUD_QA'! LEDAPS quality conditions are wrong! \n",
    "        img_scale = 1\n",
    "        img_offset = 0\n",
    "        scale_m = 30  # Resolution for B1-B7 from \"LANDSAT_LC05_C02_T1_TOA\"\n",
    "        bands4rgb_dict = {'Red':'B3', 'Green':'B2', 'Blue':'B1'}\n",
    "        save_dir = os.path.join(proj_dir, out_dir)\n",
    "    elif (sat_code == 'L7'):\n",
    "        # Landsat-7 L1 (TOA)\n",
    "        # 1999-05-28T01:02:17Z–present\n",
    "        ee_product = \"LANDSAT/LE07/C02/T1_TOA\"\n",
    "        bands_list = ['B1', 'B2', 'B3', 'B4', 'B5','B7']  # Optical bands\n",
    "        qa_bands_list = ['QA_PIXEL','QA_RADSAT']\n",
    "        img_scale = 1\n",
    "        img_offset = 0\n",
    "        scale_m = 30  # Resolution for B1-B5 from \"LANDSAT_LC07_C02_T1_TOA\"\n",
    "        bands4rgb_dict = {'Red':'B4', 'Green':'B3', 'Blue':'B2'}\n",
    "        save_dir = os.path.join(proj_dir, out_dir)\n",
    "    elif (sat_code == 'L8'):\n",
    "        # Landsat-8 L1 (TOA)\n",
    "        # 2013-03-18T15:58:14Z–present\n",
    "        ee_product = \"LANDSAT/LC08/C02/T1_TOA\"\n",
    "        bands_list = ['B1', 'B2', 'B3', 'B4', 'B5','B6','B7']  # Optical bands\n",
    "        qa_bands_list = ['QA_PIXEL','QA_RADSAT']\n",
    "        img_scale = 1\n",
    "        img_offset = 0\n",
    "        scale_m = 30  # Resolution for B1-B7 from \"LANDSAT_LC08_C02_T1_TOA\"\n",
    "        bands4rgb_dict = {'Red':'B4', 'Green':'B3', 'Blue':'B2'}\n",
    "        save_dir = os.path.join(proj_dir, out_dir)\n",
    "    elif (sat_code == 'L9'):\n",
    "        # Landsat-9 L1 (TOA)\n",
    "        # 2021-10-31T00:00:00Z–present\n",
    "        ee_product = \"LANDSAT/LC09/C02/T1_TOA\"\n",
    "        bands_list = ['B1', 'B2', 'B3', 'B4', 'B5','B6','B7']  # Optical bands\n",
    "        qa_bands_list = ['QA_PIXEL','QA_RADSAT']\n",
    "        img_scale = 1\n",
    "        img_offset = 0\n",
    "        scale_m = 30  # Resolution for B1-B7 from \"LANDSAT_LC09_C02_T1_TOA\"\n",
    "        bands4rgb_dict = {'Red':'B4', 'Green':'B3', 'Blue':'B2'}\n",
    "        save_dir = os.path.join(proj_dir, out_dir)\n",
    "    elif (sat_code == 'S2'):    \n",
    "        # Sentinel-2 L1 (TOA)\n",
    "        # 2017-03-28T00:00:00Z - present\n",
    "        ee_product = \"COPERNICUS/S2_HARMONIZED\"  # L1C-TOA\n",
    "        bands_list = ['B1', 'B2', 'B3', 'B4', 'B5','B6','B7','B8','B8A','B9','B10','B11','B12']  # Optical bands\n",
    "        qa_bands_list = ['QA60'] \n",
    "        img_scale = 0.0001\n",
    "        img_offset = 0\n",
    "        scale_m = 10  \n",
    "        bands4rgb_dict = {'Red':'B4', 'Green':'B3', 'Blue':'B2'}\n",
    "        save_dir = os.path.join(proj_dir, out_dir)\n",
    "    else:\n",
    "        print('Wrong satellite code %s' % sat_code)\n",
    "        ee_product = \"\"  \n",
    "        bands_list = []  # Optical bands\n",
    "        qa_bands_list = [] \n",
    "        img_scale = 1\n",
    "        img_offset = 0\n",
    "        scale_m = 30  \n",
    "        bands4rgb_dict = {}\n",
    "        save_dir = ''\n",
    "    return ee_product,bands_list,qa_bands_list,img_scale,img_offset,scale_m,bands4rgb_dict,save_dir\n",
    "\n",
    "# Transform EE date to str ```YYYYMMdd'T'HHmmss```, which wiill be used in file name\n",
    "def date_ee2str(date_ee, date_format=\"YYYYMMdd'T'HHmmss\"):\n",
    "    date_str = ee.Date(date_ee).format(date_format).getInfo()\n",
    "    return date_str\n",
    "\n",
    "# Transform EE date to python ```datetime```\n",
    "def date_ee2datetime(date_ee):\n",
    "    date_time = datetime.datetime.fromtimestamp(date_ee/1000, tz=pytz.UTC)\n",
    "    return date_time\n",
    "\n",
    "# Mask clouds for Landsat sensors \n",
    "def img_mask_L5789(image):\n",
    "    QA_RADSAT = image.select('QA_RADSAT')\n",
    "    radsatMask = QA_RADSAT.bitwiseAnd(0b111111101001).eq(0)\n",
    "    QA_PIXEL = image.select('QA_PIXEL')\n",
    "    fillMask = QA_PIXEL.bitwiseAnd(1<<0).eq(0)  # Image data\n",
    "    dilatedCloudMask = QA_PIXEL.bitwiseAnd(1<<1).eq(0)  # Dilated Cloud\n",
    "    cirrusMask = QA_PIXEL.bitwiseAnd(1<<2).eq(0)  # Cirrus\n",
    "    cloudMask = QA_PIXEL.bitwiseAnd(1<<3).eq(0)  # Cloud\n",
    "    cloudShadowMask = QA_PIXEL.bitwiseAnd(1<<4).eq(0)  # Cloud shadow\n",
    "    maskTot = radsatMask.And(dilatedCloudMask).And(cirrusMask).And(cloudMask).And(cloudShadowMask).rename('mask')\n",
    "    img_masked = image.updateMask(maskTot)\n",
    "    #\n",
    "    return img_masked\n",
    "\n",
    "# Mask clouds for Sentinel-2 sensor using opaqueCloudsMask and cirrusCloudsMask\n",
    "def img_mask_S2(image):\n",
    "    if ('QA60' in qa_bands_list):\n",
    "        QA60 = image.select('QA60')\n",
    "        opaqueCloudsMask = QA60.bitwiseAnd(1<<10).eq(0)\n",
    "        cirrusCloudsMask = QA60.bitwiseAnd(1<<11).eq(0)\n",
    "    else:\n",
    "        opaqueCloudsMask = ee.Image(1)\n",
    "        cirrusCloudsMask = ee.Image(1)\n",
    "    #\n",
    "    maskTot = opaqueCloudsMask.And(cirrusCloudsMask).rename('mask')\n",
    "    img_masked = image.updateMask(maskTot)\n",
    "    #\n",
    "    return img_masked\n",
    "\n",
    "# Calculate the number of images available for each sample\n",
    "def calcNimg(feature):\n",
    "    dateRange4stn = ee.DateRange(ee.Date(feature.get('date')).advance(-delta_days,'day'), \\\n",
    "                    ee.Date(feature.get('date')).advance(delta_days,'day'))\n",
    "    imgColl4stn = imgCollMasked.filterDate(dateRange4stn.start(), dateRange4stn.end()) \\\n",
    "                .filterBounds(feature.geometry()) \\\n",
    "                .map(detectar_duplicador).filter(ee.Filter.eq(\"duplicate\",\"no duplicate\"))\n",
    "    feature = feature.set({'nImg': imgColl4stn.size()})\n",
    "    return feature\n",
    "\n",
    "# Remove images with duplicated dates\n",
    "# From: https://gis.stackexchange.com/questions/336257/filter-out-duplicate-sentinel-2-images-form-earth-engine-image-collection-by-dat\n",
    "def detectar_duplicador(image):\n",
    "    esduplicado = ee.String(\"\")\n",
    "    numero = eeImgCollList.indexOf(image)\n",
    "    image1 = ee.Image(eeImgCollList.get(numero.add(1)))\n",
    "    # Compare the image(0) in the ImageCollection with the image(1) in the List\n",
    "    fecha1 = image.date().format(\"Y-M-d\")\n",
    "    fecha2 = image1.date().format(\"Y-M-d\")\n",
    "    estado = ee.Algorithms.IsEqual(fecha1,fecha2)\n",
    "    esduplicado = ee.String(ee.Algorithms.If(estado, \"duplicate\", \"no duplicate\"))\n",
    "    return image.set({\"duplicate\": esduplicado})\n",
    "\n",
    "# Calculate mean and Std.Dev of Rrs of <bands_list> in the circular region around the station location\n",
    "def calc_RrsMeanStd(feature):\n",
    "    # Filter the images collection within the date range\n",
    "    dateRange4stn = ee.DateRange(ee.Date(feature.get('date')).advance(-delta_days,'day'), \\\n",
    "                    ee.Date(feature.get('date')).advance(delta_days,'day'))\n",
    "    # Add to each image a band 'days_lag' and sort the collection \n",
    "    def add_days_lag(image):\n",
    "        lagDays = ee.Number(ee.Date(image.get(\"system:time_start\")) \\\n",
    "                     .difference(start=ee.Date(feature.get('date')), unit='day'))\n",
    "        lagAbsDays = lagDays.abs()\n",
    "        image = image.set({'lagAbsDays':lagAbsDays}) \\\n",
    "            .addBands(ee.Image.constant(lagDays).toFloat().rename('lagDays'))\n",
    "        return image\n",
    "    #\n",
    "    imgColl4stn = imgCollMasked.filterDate(dateRange4stn.start(), dateRange4stn.end()) \\\n",
    "                .filterBounds(feature.geometry()) \\\n",
    "                .map(detectar_duplicador).filter(ee.Filter.eq(\"duplicate\",\"no duplicate\")) \\\n",
    "                .map(add_days_lag).sort('lagAbsDays',False) \n",
    "    #\n",
    "    reducerMeanStDev = ee.Reducer.mean().combine(ee.Reducer.stdDev(),'',True)\n",
    "    sampleGeom = ee.Geometry.Point([feature.getNumber('longitude'), feature.getNumber('latitude')]) \\\n",
    "                                    .buffer(bufferR, maxError=1) \\\n",
    "                    .intersection(feature.geometry().buffer(bufferS, maxError=1), maxError=1)\n",
    "    feature_means = imgColl4stn.select(bands_list + ['lagDays']) \\\n",
    "                .mosaic().reduceRegion(**{ \\\n",
    "                      'reducer': reducerMeanStDev, \\\n",
    "                      'geometry': sampleGeom, \\\n",
    "                      'scale': scale_m, \\\n",
    "                      'maxPixels': 1e19 \\\n",
    "                })\n",
    "    return feature.set(feature_means)\n",
    "\n",
    "# Calculate average Rrs \n",
    "def calcRrsIn(feature):\n",
    "    feature = feature.set({'RrsIn': feature.propertyNames().contains(bands_list[0]+'_mean')})\n",
    "    return feature\n",
    "\n",
    "# Add the product ID \n",
    "def add_product_id(feature):\n",
    "    date_range = ee.Date(feature.get('date')).advance(feature.get('lagDays_mean'),'day').getRange('day')\n",
    "    productIdStr = ee.Algorithms.If(condition=ee.String(sat_code).equals('S2'), \\\n",
    "                                    trueCase='PRODUCT_ID', falseCase='LANDSAT_PRODUCT_ID')\n",
    "    PRODUCT_ID = eeImgColl.filterDate(date_range.start(),date_range.end()).first().get(productIdStr)\n",
    "    return feature.set({'PRODUCT_ID': PRODUCT_ID})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db8c08d",
   "metadata": {},
   "source": [
    "**Step 4: Read in input data, including the CCC geodatabase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a219ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in geodatabase \n",
    "ccc_ponds_gdf = gpd.read_file(os.path.join(proj_dir,ccc_ponds_fn))\n",
    "# Drop the objects without CCC_GIS_ID and Shape_Area\n",
    "ccc_ponds_gdf.dropna(axis=0, how='any', subset=['CCC_GIS_ID','Shape_Area'], inplace=True)\n",
    "ccc_ponds_gdf['CCC_GIS_ID'] = [s.strip() for s in ccc_ponds_gdf['CCC_GIS_ID']]\n",
    "ccc_ponds_gdf = ccc_ponds_gdf.loc[ccc_ponds_gdf['CCC_GIS_ID']!='']\n",
    "# Sort the objects\n",
    "ccc_ponds_gdf.sort_values(by='Shape_Area', inplace=True, ascending=False, ignore_index=True)\n",
    "# Drop the objects with area <1 ha\n",
    "area_min = 10000 # 1 ha\n",
    "ccc_ponds_gdf = ccc_ponds_gdf.loc[ccc_ponds_gdf['Shape_Area']>=area_min]\n",
    "# Transform to EPSG:4326\n",
    "ccc_ponds_gdf = ccc_ponds_gdf.to_crs('EPSG:4326')\n",
    "# Transform multipolygons to polygons\n",
    "ccc_ponds_gdf=ccc_ponds_gdf.explode(index_parts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55b65a8",
   "metadata": {},
   "source": [
    "**Step 5: For each satellite, extract spectral information at the center of each pond and export as a CSV file. First, set parameters used for each satellite sensor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b269259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The buffer around each sampling point (m)\n",
    "bufferR = 10\n",
    "# The buffer offshore (m, set to less than 0)\n",
    "bufferS = -10\n",
    "# The number of stations to process\n",
    "nStn2process = 10 \n",
    "# The maximum time lag between sample and image\n",
    "delta_days = 1  \n",
    "# Cloud cover threshold (%)\n",
    "cloud_cover_threshold = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363bc3ef",
   "metadata": {},
   "source": [
    "**<em>Sentinel-2 (S2) which was launched 28 March 2017 and is still operational.</em>** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3ac58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the satellite code\n",
    "sat_code = 'S2'\n",
    "\n",
    "# Loop through each pond in ccc_ponds_gdf and extract satellite data \n",
    "for k_pond in range(0,len(ccc_ponds_gdf)):\n",
    "    ccc_ponds_gdfSel = ccc_ponds_gdf.iloc[k_pond]\n",
    "\n",
    "    # Calculate the pond center - the most offshore location\n",
    "    pond_geom = ccc_ponds_gdfSel['geometry']\n",
    "    max_offshore_point = findPmaxOffshore(pond_geom)\n",
    "    ee_product,bands_list,qa_bands_list,img_scale,img_offset,scale_m,bands4rgb_dict,save_dir = set_satellite(sat_code)\n",
    "    colNames = ['PRODUCT_ID','CCC_GIS_ID','day_str','lagDays_mean','lagDays_stdDev','latitude','longitude'] + list(np.array([[s+'_mean',s+'_stdDev'] for s in bands_list]).flatten())\n",
    "    save_fn = '%03d_%s_R%03d.csv' % (k_pond, ccc_ponds_gdfSel['CCC_GIS_ID'], bufferR)\n",
    "    roi_ee = ee.Geometry.Point([float(max_offshore_point.x.item()), float(max_offshore_point.y.item())]).buffer(bufferR)\n",
    "    ee_img_coll = ee.ImageCollection(ee_product).filterDate(date_start, date_end).filterBounds(roi_ee).sort(\"system:time_start\").select(bands_list + qa_bands_list)\n",
    "    \n",
    "    # The number of images in the collection\n",
    "    ee_img_count = ee_img_coll.size().getInfo()\n",
    "    ee_img_collA = ee_img_coll.filter(ee.Filter.lt('CLOUD_COVERAGE_ASSESSMENT', cloud_cover_threshold))\n",
    "    img_dates_df = pd.DataFrame()\n",
    "    img_dates_df['date_ee'] = ee_img_collA.aggregate_array(\"system:time_start\").getInfo()\n",
    "    img_dates_df['date_time'] = [date_ee2datetime(date_ee) for date_ee in img_dates_df['date_ee']]\n",
    "    day_str = [t.strftime('%Y-%m-%d') for t in img_dates_df['date_time']]\n",
    "    stn_dfSat = pd.DataFrame(data={'day_str':day_str})\n",
    "    stn_dfSat['date'] = pd.to_datetime(stn_dfSat['day_str'])\n",
    "    stn_dfSat['CCC_GIS_ID'] = ccc_ponds_gdfSel['CCC_GIS_ID']\n",
    "    stn_dfSat['latitude'] = float(max_offshore_point.y.item())\n",
    "    stn_dfSat['longitude'] = float(max_offshore_point.x.item())\n",
    "    stn_dfSat['Pond Name'] = ccc_ponds_gdfSel['NAME']\n",
    "    nStn = len(stn_dfSat)\n",
    "    ccc_ponds_gdf1 = ccc_ponds_gdf[['CCC_GIS_ID','geometry']].copy()\n",
    "    indx_start = 0; indx_end = indx_start+nStn2process\n",
    "    rrs_df = pd.DataFrame()\n",
    "    \n",
    "    # Loop through each station \n",
    "    while (indx_start<nStn):\n",
    "        stn_df1 = stn_dfSat[indx_start:indx_end].copy()\n",
    "        stn_gdf = gpd.GeoDataFrame(stn_df1.merge(ccc_ponds_gdf1, how='left', left_on='CCC_GIS_ID', right_on='CCC_GIS_ID').dropna())\n",
    "        # Create feature collection\n",
    "        stnFCwPolygons = geemap.geopandas_to_ee(stn_gdf, date='day_str', date_format='YYYY-MM-dd')\n",
    "        eeImgColl = ee.ImageCollection(ee_product).filterDate(ee.Date(str(np.min(stn_gdf['date']).strftime('%Y-%m-%d'))).advance(-delta_days,'day'),ee.Date(str(np.max(stn_gdf['date']).strftime('%Y-%m-%d'))).advance(delta_days,'day')).filterBounds(stnFCwPolygons).sort(\"system:time_start\").select(bands_list + qa_bands_list)\n",
    "        # Apply cloud mask \n",
    "        imgCollMasked = eeImgColl.map(lambda image: img_mask_S2(image)).select(bands_list).map(lambda image: image.multiply(img_scale).add(img_offset).copyProperties(image, ['system:time_start']))\n",
    "        # Generate a List to compare dates\n",
    "        eeImgCollList = eeImgColl.toList(eeImgColl.size())\n",
    "        image = ee.Image(eeImgCollList.get(0))\n",
    "        # Add in the end of the list a dummy image\n",
    "        eeImgCollList = eeImgCollList.add(image)\n",
    "        stnWithImgFc = stnFCwPolygons.map(calcNimg).filter(ee.Filter.gt('nImg',0)).map(calc_RrsMeanStd).map(calcRrsIn).filter(ee.Filter.eq('RrsIn',True)).map(add_product_id)\n",
    "        if (stnWithImgFc.size().getInfo() > 0):\n",
    "            stnWithImgDf = geemap.ee_to_pandas(stnWithImgFc, col_names=colNames)\n",
    "            stnWithImgDf = stnWithImgDf.dropna().reset_index(drop=True)\n",
    "            rrs_df = pd.concat([rrs_df,stnWithImgDf])\n",
    "        indx_start = indx_end; indx_end = indx_start + nStn2process\n",
    "    print('    Rrs in: %d stations' % len(rrs_df))\n",
    "    \n",
    "    # Export to csv\n",
    "    saveSubDir = os.path.join(save_dir, 'S2/')\n",
    "    if (not os.path.exists(saveSubDir)):\n",
    "        os.mkdir(saveSubDir)\n",
    "    rrs_df.to_csv(os.path.join(saveSubDir,save_fn), index=False)\n",
    "    print('    Exported to %s' % os.path.join(saveSubDir,save_fn))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e18515",
   "metadata": {},
   "source": [
    "**<em>Landsat 5 (L5) was operational from 16 March 1984 to 5 May 2012.</em>** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16144c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the satellite code\n",
    "sat_code = 'L5'\n",
    "\n",
    "# Loop through each pond in ccc_ponds_gdf and extract satellite data \n",
    "for k_pond in range(0,len(ccc_ponds_gdf)):\n",
    "    ccc_ponds_gdfSel = ccc_ponds_gdf.iloc[k_pond]\n",
    "    \n",
    "    # Calculate the pond center - the most offshore location\n",
    "    pond_geom = ccc_ponds_gdfSel['geometry']\n",
    "    max_offshore_point = findPmaxOffshore(pond_geom)\n",
    "    ee_product,bands_list,qa_bands_list,img_scale,img_offset,scale_m,bands4rgb_dict,save_dir = set_satellite(sat_code)\n",
    "    colNames = ['PRODUCT_ID','CCC_GIS_ID','day_str','lagDays_mean','lagDays_stdDev','latitude','longitude'] + list(np.array([[s+'_mean',s+'_stdDev'] for s in bands_list]).flatten())\n",
    "    save_fn = '%03d_%s_R%03d.csv' % (k_pond, ccc_ponds_gdfSel['CCC_GIS_ID'], bufferR)\n",
    "    roi_ee = ee.Geometry.Point([float(max_offshore_point.x.item()), float(max_offshore_point.y.item())]).buffer(bufferR)\n",
    "    ee_img_coll = ee.ImageCollection(ee_product).filterDate(date_start, date_end).filterBounds(roi_ee).sort(\"system:time_start\").select(bands_list + qa_bands_list)\n",
    "    \n",
    "    # The number of images in the collection\n",
    "    ee_img_count = ee_img_coll.size().getInfo()\n",
    "    ee_img_collA = ee_img_coll.filter(ee.Filter.lt('CLOUD_COVER', cloud_cover_threshold))\n",
    "    img_dates_df = pd.DataFrame()\n",
    "    img_dates_df['date_ee'] = ee_img_collA.aggregate_array(\"system:time_start\").getInfo()\n",
    "    img_dates_df['date_time'] = [date_ee2datetime(date_ee) for date_ee in img_dates_df['date_ee']]\n",
    "    day_str = [t.strftime('%Y-%m-%d') for t in img_dates_df['date_time']]\n",
    "    stn_dfSat = pd.DataFrame(data={'day_str':day_str})\n",
    "    stn_dfSat['date'] = pd.to_datetime(stn_dfSat['day_str'])\n",
    "    stn_dfSat['CCC_GIS_ID'] = ccc_ponds_gdfSel['CCC_GIS_ID']\n",
    "    stn_dfSat['latitude'] = float(max_offshore_point.y.item())\n",
    "    stn_dfSat['longitude'] = float(max_offshore_point.x.item())\n",
    "    stn_dfSat['Pond Name'] = ccc_ponds_gdfSel['NAME']\n",
    "    nStn = len(stn_dfSat)\n",
    "    ccc_ponds_gdf1 = ccc_ponds_gdf[['CCC_GIS_ID','geometry']].copy()\n",
    "    indx_start = 0; indx_end = indx_start+nStn2process\n",
    "    rrs_df = pd.DataFrame()\n",
    "    \n",
    "    # Loop through each station \n",
    "    while (indx_start<nStn):\n",
    "        stn_df1 = stn_dfSat[indx_start:indx_end].copy()\n",
    "        stn_gdf = gpd.GeoDataFrame(stn_df1.merge(ccc_ponds_gdf1, how='left', left_on='CCC_GIS_ID', right_on='CCC_GIS_ID').dropna())\n",
    "        # Create feature collection\n",
    "        stnFCwPolygons = geemap.geopandas_to_ee(stn_gdf, date='day_str', date_format='YYYY-MM-dd')\n",
    "        eeImgColl = ee.ImageCollection(ee_product).filterDate(ee.Date(str(np.min(stn_gdf['date']).strftime('%Y-%m-%d'))).advance(-delta_days,'day'), ee.Date(str(np.max(stn_gdf['date']).strftime('%Y-%m-%d'))).advance(delta_days,'day')).filterBounds(stnFCwPolygons).sort(\"system:time_start\").select(bands_list + qa_bands_list)\n",
    "        # Apply cloud mask \n",
    "        imgCollMasked = eeImgColl.map(lambda image: img_mask_L5789(image)).select(bands_list).map(lambda image: image.multiply(img_scale).add(img_offset).copyProperties(image, ['system:time_start']))\n",
    "        # Generate a List to compare dates\n",
    "        eeImgCollList = eeImgColl.toList(eeImgColl.size())\n",
    "        image = ee.Image(eeImgCollList.get(0))\n",
    "        # Add in the end of the list a dummy image\n",
    "        eeImgCollList = eeImgCollList.add(image)\n",
    "        stnWithImgFc = stnFCwPolygons.map(calcNimg).filter(ee.Filter.gt('nImg',0)).map(calc_RrsMeanStd).map(calcRrsIn).filter(ee.Filter.eq('RrsIn',True)).map(add_product_id)\n",
    "        if (stnWithImgFc.size().getInfo() > 0):\n",
    "            stnWithImgDf = geemap.ee_to_pandas(stnWithImgFc, col_names=colNames)\n",
    "            stnWithImgDf = stnWithImgDf.dropna().reset_index(drop=True)\n",
    "            rrs_df = pd.concat([rrs_df,stnWithImgDf])\n",
    "        indx_start = indx_end; indx_end = indx_start + nStn2process\n",
    "    print('    Rrs in: %d stations' % len(rrs_df))\n",
    "    \n",
    "    # Export to csv\n",
    "    saveSubDir = os.path.join(save_dir, 'L5/')\n",
    "    if (not os.path.exists(saveSubDir)):\n",
    "        os.mkdir(saveSubDir)\n",
    "    rrs_df.to_csv(os.path.join(saveSubDir,save_fn), index=False)\n",
    "    print('    Exported to %s' % os.path.join(saveSubDir,save_fn))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842373ed",
   "metadata": {},
   "source": [
    "**<em>Landsat 7 (L7) was operational from 28 May 1999 to 6 April 2022.</em>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bfe140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the satellite code\n",
    "sat_code = 'L7'\n",
    "\n",
    "# Loop through each pond in ccc_ponds_gdf and extract satellite data \n",
    "for k_pond in range(0,len(ccc_ponds_gdf)):\n",
    "    ccc_ponds_gdfSel = ccc_ponds_gdf.iloc[k_pond]\n",
    "    \n",
    "    # Calculate the pond center - the most offshore location\n",
    "    pond_geom = ccc_ponds_gdfSel['geometry']\n",
    "    max_offshore_point = findPmaxOffshore(pond_geom)\n",
    "    ee_product,bands_list,qa_bands_list,img_scale,img_offset,scale_m,bands4rgb_dict,save_dir = set_satellite(sat_code)\n",
    "    colNames = ['PRODUCT_ID','CCC_GIS_ID','day_str','lagDays_mean','lagDays_stdDev','latitude','longitude'] + list(np.array([[s+'_mean',s+'_stdDev'] for s in bands_list]).flatten())\n",
    "    save_fn = '%03d_%s_R%03d.csv' % (k_pond, ccc_ponds_gdfSel['CCC_GIS_ID'], bufferR)\n",
    "    roi_ee = ee.Geometry.Point([float(max_offshore_point.x.item()), float(max_offshore_point.y.item())]).buffer(bufferR)\n",
    "    ee_img_coll = ee.ImageCollection(ee_product).filterDate(date_start, date_end).filterBounds(roi_ee).sort(\"system:time_start\").select(bands_list + qa_bands_list)\n",
    "    \n",
    "    # The number of images in the collection\n",
    "    ee_img_count = ee_img_coll.size().getInfo()\n",
    "    ee_img_collA = ee_img_coll.filter(ee.Filter.lt('CLOUD_COVER', cloud_cover_threshold))\n",
    "    img_dates_df = pd.DataFrame()\n",
    "    img_dates_df['date_ee'] = ee_img_collA.aggregate_array(\"system:time_start\").getInfo()\n",
    "    img_dates_df['date_time'] = [date_ee2datetime(date_ee) for date_ee in img_dates_df['date_ee']]\n",
    "    day_str = [t.strftime('%Y-%m-%d') for t in img_dates_df['date_time']]\n",
    "    stn_dfSat = pd.DataFrame(data={'day_str':day_str})\n",
    "    stn_dfSat['date'] = pd.to_datetime(stn_dfSat['day_str'])\n",
    "    stn_dfSat['CCC_GIS_ID'] = ccc_ponds_gdfSel['CCC_GIS_ID']\n",
    "    stn_dfSat['latitude'] = float(max_offshore_point.y.item())\n",
    "    stn_dfSat['longitude'] = float(max_offshore_point.x.item())\n",
    "    stn_dfSat['Pond Name'] = ccc_ponds_gdfSel['NAME']\n",
    "    nStn = len(stn_dfSat)\n",
    "    ccc_ponds_gdf1 = ccc_ponds_gdf[['CCC_GIS_ID','geometry']].copy()\n",
    "    indx_start = 0; indx_end = indx_start+nStn2process\n",
    "    rrs_df = pd.DataFrame()\n",
    "    \n",
    "    # Loop through each station \n",
    "    while (indx_start<nStn):\n",
    "        stn_df1 = stn_dfSat[indx_start:indx_end].copy()\n",
    "        stn_gdf = gpd.GeoDataFrame(stn_df1.merge(ccc_ponds_gdf1, how='left', left_on='CCC_GIS_ID', right_on='CCC_GIS_ID').dropna())\n",
    "        # Create feature collection\n",
    "        stnFCwPolygons = geemap.geopandas_to_ee(stn_gdf, date='day_str', date_format='YYYY-MM-dd')\n",
    "        eeImgColl = ee.ImageCollection(ee_product).filterDate(ee.Date(str(np.min(stn_gdf['date']).strftime('%Y-%m-%d'))).advance(-delta_days,'day'), ee.Date(str(np.max(stn_gdf['date']).strftime('%Y-%m-%d'))).advance(delta_days,'day')).filterBounds(stnFCwPolygons).sort(\"system:time_start\").select(bands_list + qa_bands_list)\n",
    "        # Apply cloud mask \n",
    "        imgCollMasked = eeImgColl.map(lambda image: img_mask_L5789(image)).select(bands_list).map(lambda image: image.multiply(img_scale).add(img_offset).copyProperties(image, ['system:time_start']))\n",
    "        # Generate a List to compare dates\n",
    "        eeImgCollList = eeImgColl.toList(eeImgColl.size())\n",
    "        image = ee.Image(eeImgCollList.get(0))\n",
    "        # Add in the end of the list a dummy image\n",
    "        eeImgCollList = eeImgCollList.add(image)\n",
    "        stnWithImgFc = stnFCwPolygons.map(calcNimg).filter(ee.Filter.gt('nImg',0)).map(calc_RrsMeanStd).map(calcRrsIn).filter(ee.Filter.eq('RrsIn',True)).map(add_product_id)\n",
    "        if (stnWithImgFc.size().getInfo() > 0):\n",
    "            stnWithImgDf = geemap.ee_to_pandas(stnWithImgFc, col_names=colNames)\n",
    "            stnWithImgDf = stnWithImgDf.dropna().reset_index(drop=True)\n",
    "            rrs_df = pd.concat([rrs_df,stnWithImgDf])\n",
    "        indx_start = indx_end; indx_end = indx_start + nStn2process\n",
    "    print('    Rrs in: %d stations' % len(rrs_df))\n",
    "    \n",
    "    # Export to csv\n",
    "    saveSubDir = os.path.join(save_dir, 'L7/')\n",
    "    if (not os.path.exists(saveSubDir)):\n",
    "        os.mkdir(saveSubDir)\n",
    "    rrs_df.to_csv(os.path.join(saveSubDir,save_fn), index=False)\n",
    "    print('    Exported to %s' % os.path.join(saveSubDir,save_fn))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3292b04",
   "metadata": {},
   "source": [
    "**<em>Landsat 8 (L8) which was launched 18 March 2013 and is still operational.</em>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8f3e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the satellite code\n",
    "sat_code = 'L8'\n",
    "\n",
    "# Loop through each pond in ccc_ponds_gdf and extract satellite data \n",
    "for k_pond in range(0,len(ccc_ponds_gdf)):\n",
    "    ccc_ponds_gdfSel = ccc_ponds_gdf.iloc[k_pond]\n",
    "    \n",
    "    # Calculate the pond center - the most offshore location\n",
    "    pond_geom = ccc_ponds_gdfSel['geometry']\n",
    "    max_offshore_point = findPmaxOffshore(pond_geom)\n",
    "    ee_product,bands_list,qa_bands_list,img_scale,img_offset,scale_m,bands4rgb_dict,save_dir = set_satellite(sat_code)\n",
    "    colNames = ['PRODUCT_ID','CCC_GIS_ID','day_str','lagDays_mean','lagDays_stdDev','latitude','longitude'] + list(np.array([[s+'_mean',s+'_stdDev'] for s in bands_list]).flatten())\n",
    "    save_fn = '%03d_%s_R%03d.csv' % (k_pond, ccc_ponds_gdfSel['CCC_GIS_ID'], bufferR)\n",
    "    roi_ee = ee.Geometry.Point([float(max_offshore_point.x.item()), float(max_offshore_point.y.item())]).buffer(bufferR)\n",
    "    ee_img_coll = ee.ImageCollection(ee_product).filterDate(date_start, date_end).filterBounds(roi_ee).sort(\"system:time_start\").select(bands_list + qa_bands_list)\n",
    "    \n",
    "    # The number of images in the collection\n",
    "    ee_img_count = ee_img_coll.size().getInfo()\n",
    "    ee_img_collA = ee_img_coll.filter(ee.Filter.lt('CLOUD_COVER', cloud_cover_threshold))\n",
    "    img_dates_df = pd.DataFrame()\n",
    "    img_dates_df['date_ee'] = ee_img_collA.aggregate_array(\"system:time_start\").getInfo()\n",
    "    img_dates_df['date_time'] = [date_ee2datetime(date_ee) for date_ee in img_dates_df['date_ee']]\n",
    "    day_str = [t.strftime('%Y-%m-%d') for t in img_dates_df['date_time']]\n",
    "    stn_dfSat = pd.DataFrame(data={'day_str':day_str})\n",
    "    stn_dfSat['date'] = pd.to_datetime(stn_dfSat['day_str'])\n",
    "    stn_dfSat['CCC_GIS_ID'] = ccc_ponds_gdfSel['CCC_GIS_ID']\n",
    "    stn_dfSat['latitude'] = float(max_offshore_point.y.item())\n",
    "    stn_dfSat['longitude'] = float(max_offshore_point.x.item())\n",
    "    stn_dfSat['Pond Name'] = ccc_ponds_gdfSel['NAME']\n",
    "    nStn = len(stn_dfSat)\n",
    "    ccc_ponds_gdf1 = ccc_ponds_gdf[['CCC_GIS_ID','geometry']].copy()\n",
    "    indx_start = 0; indx_end = indx_start+nStn2process\n",
    "    rrs_df = pd.DataFrame()\n",
    "    \n",
    "    # Loop through each station \n",
    "    while (indx_start<nStn):\n",
    "        stn_df1 = stn_dfSat[indx_start:indx_end].copy()\n",
    "        stn_gdf = gpd.GeoDataFrame(stn_df1.merge(ccc_ponds_gdf1, how='left', left_on='CCC_GIS_ID', right_on='CCC_GIS_ID').dropna())\n",
    "        # Create feature collection\n",
    "        stnFCwPolygons = geemap.geopandas_to_ee(stn_gdf, date='day_str', date_format='YYYY-MM-dd')\n",
    "        eeImgColl = ee.ImageCollection(ee_product).filterDate(ee.Date(str(np.min(stn_gdf['date']).strftime('%Y-%m-%d'))).advance(-delta_days,'day'), ee.Date(str(np.max(stn_gdf['date']).strftime('%Y-%m-%d'))).advance(delta_days,'day')).filterBounds(stnFCwPolygons).sort(\"system:time_start\").select(bands_list + qa_bands_list)\n",
    "        # Apply cloud mask \n",
    "        imgCollMasked = eeImgColl.map(lambda image: img_mask_L5789(image)).select(bands_list).map(lambda image: image.multiply(img_scale).add(img_offset).copyProperties(image, ['system:time_start']))\n",
    "        # Generate a List to compare dates\n",
    "        eeImgCollList = eeImgColl.toList(eeImgColl.size())\n",
    "        image = ee.Image(eeImgCollList.get(0))\n",
    "        # Add in the end of the list a dummy image\n",
    "        eeImgCollList = eeImgCollList.add(image)\n",
    "        stnWithImgFc = stnFCwPolygons.map(calcNimg).filter(ee.Filter.gt('nImg',0)).map(calc_RrsMeanStd).map(calcRrsIn).filter(ee.Filter.eq('RrsIn',True)).map(add_product_id)\n",
    "        if (stnWithImgFc.size().getInfo() > 0):\n",
    "            stnWithImgDf = geemap.ee_to_pandas(stnWithImgFc, col_names=colNames)\n",
    "            stnWithImgDf = stnWithImgDf.dropna().reset_index(drop=True)\n",
    "            rrs_df = pd.concat([rrs_df,stnWithImgDf])\n",
    "        indx_start = indx_end; indx_end = indx_start + nStn2process\n",
    "    print('    Rrs in: %d stations' % len(rrs_df))\n",
    "    \n",
    "    # Export to csv\n",
    "    saveSubDir = os.path.join(save_dir, 'L8/')\n",
    "    if (not os.path.exists(saveSubDir)):\n",
    "        os.mkdir(saveSubDir)\n",
    "    rrs_df.to_csv(os.path.join(saveSubDir,save_fn), index=False)\n",
    "    print('    Exported to %s' % os.path.join(saveSubDir,save_fn))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a95284",
   "metadata": {},
   "source": [
    "**<em>Landsat 9 (L9) which was launched 31 October 2021 and is still operational.</em>** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd364ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the satellite code\n",
    "sat_code = 'L9'\n",
    "\n",
    "# Loop through each pond in ccc_ponds_gdf and extract satellite data \n",
    "for k_pond in range(0,len(ccc_ponds_gdf)):\n",
    "    ccc_ponds_gdfSel = ccc_ponds_gdf.iloc[k_pond]\n",
    "    \n",
    "    # Calculate the pond center - the most offshore location\n",
    "    pond_geom = ccc_ponds_gdfSel['geometry']\n",
    "    max_offshore_point = findPmaxOffshore(pond_geom)\n",
    "    ee_product,bands_list,qa_bands_list,img_scale,img_offset,scale_m,bands4rgb_dict,save_dir = set_satellite(sat_code)\n",
    "    colNames = ['PRODUCT_ID','CCC_GIS_ID','day_str','lagDays_mean','lagDays_stdDev','latitude','longitude'] + list(np.array([[s+'_mean',s+'_stdDev'] for s in bands_list]).flatten())\n",
    "    save_fn = '%03d_%s_R%03d.csv' % (k_pond, ccc_ponds_gdfSel['CCC_GIS_ID'], bufferR)\n",
    "    roi_ee = ee.Geometry.Point([float(max_offshore_point.x.item()), float(max_offshore_point.y.item())]).buffer(bufferR)\n",
    "    ee_img_coll = ee.ImageCollection(ee_product).filterDate(date_start, date_end).filterBounds(roi_ee).sort(\"system:time_start\").select(bands_list + qa_bands_list)\n",
    "    \n",
    "    # The number of images in the collection\n",
    "    ee_img_count = ee_img_coll.size().getInfo()\n",
    "    ee_img_collA = ee_img_coll.filter(ee.Filter.lt('CLOUD_COVER', cloud_cover_threshold))\n",
    "    img_dates_df = pd.DataFrame()\n",
    "    img_dates_df['date_ee'] = ee_img_collA.aggregate_array(\"system:time_start\").getInfo()\n",
    "    img_dates_df['date_time'] = [date_ee2datetime(date_ee) for date_ee in img_dates_df['date_ee']]\n",
    "    day_str = [t.strftime('%Y-%m-%d') for t in img_dates_df['date_time']]\n",
    "    stn_dfSat = pd.DataFrame(data={'day_str':day_str})\n",
    "    stn_dfSat['date'] = pd.to_datetime(stn_dfSat['day_str'])\n",
    "    stn_dfSat['CCC_GIS_ID'] = ccc_ponds_gdfSel['CCC_GIS_ID']\n",
    "    stn_dfSat['latitude'] = float(max_offshore_point.y.item())\n",
    "    stn_dfSat['longitude'] = float(max_offshore_point.x.item())\n",
    "    stn_dfSat['Pond Name'] = ccc_ponds_gdfSel['NAME']\n",
    "    nStn = len(stn_dfSat)\n",
    "    ccc_ponds_gdf1 = ccc_ponds_gdf[['CCC_GIS_ID','geometry']].copy()\n",
    "    indx_start = 0; indx_end = indx_start+nStn2process\n",
    "    rrs_df = pd.DataFrame()\n",
    "    \n",
    "    # Loop through each station \n",
    "    while (indx_start<nStn):\n",
    "        stn_df1 = stn_dfSat[indx_start:indx_end].copy()\n",
    "        stn_gdf = gpd.GeoDataFrame(stn_df1.merge(ccc_ponds_gdf1, how='left', left_on='CCC_GIS_ID', right_on='CCC_GIS_ID').dropna())\n",
    "        # Create feature collection\n",
    "        stnFCwPolygons = geemap.geopandas_to_ee(stn_gdf, date='day_str', date_format='YYYY-MM-dd')\n",
    "        eeImgColl = ee.ImageCollection(ee_product).filterDate(ee.Date(str(np.min(stn_gdf['date']).strftime('%Y-%m-%d'))).advance(-delta_days,'day'), ee.Date(str(np.max(stn_gdf['date']).strftime('%Y-%m-%d'))).advance(delta_days,'day')).filterBounds(stnFCwPolygons).sort(\"system:time_start\").select(bands_list + qa_bands_list)\n",
    "        # Apply cloud mask \n",
    "        imgCollMasked = eeImgColl.map(lambda image: img_mask_L5789(image)).select(bands_list).map(lambda image: image.multiply(img_scale).add(img_offset).copyProperties(image, ['system:time_start']))\n",
    "        # Generate a List to compare dates\n",
    "        eeImgCollList = eeImgColl.toList(eeImgColl.size())\n",
    "        image = ee.Image(eeImgCollList.get(0))\n",
    "        # Add in the end of the list a dummy image\n",
    "        eeImgCollList = eeImgCollList.add(image)\n",
    "        stnWithImgFc = stnFCwPolygons.map(calcNimg).filter(ee.Filter.gt('nImg',0)).map(calc_RrsMeanStd).map(calcRrsIn).filter(ee.Filter.eq('RrsIn',True)).map(add_product_id)\n",
    "        if (stnWithImgFc.size().getInfo() > 0):\n",
    "            stnWithImgDf = geemap.ee_to_pandas(stnWithImgFc, col_names=colNames)\n",
    "            stnWithImgDf = stnWithImgDf.dropna().reset_index(drop=True)\n",
    "            rrs_df = pd.concat([rrs_df,stnWithImgDf])\n",
    "        indx_start = indx_end; indx_end = indx_start + nStn2process\n",
    "    print('    Rrs in: %d stations' % len(rrs_df))\n",
    "    \n",
    "    # Export to csv\n",
    "    saveSubDir = os.path.join(save_dir, 'L9/')\n",
    "    if (not os.path.exists(saveSubDir)):\n",
    "        os.mkdir(saveSubDir)\n",
    "    rrs_df.to_csv(os.path.join(saveSubDir,save_fn), index=False)\n",
    "    print('    Exported to %s' % os.path.join(saveSubDir,save_fn))\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
